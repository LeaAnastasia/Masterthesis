{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "file_path = \"/Users/leazeiberts/Downloads/data/Brain - Amygdala_junctions.tsv\"\n",
    "metadata_file_path = \"/Users/leazeiberts/Master/Masterarbeit/age_info.tsv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "metadata = pd.read_csv(metadata_file_path, sep=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "df[['Chromosome', 'Start', 'End']] = df['Name'].str.extract(r'(chr\\w+)_(\\d+)_(\\d+)')\n",
    "df[['Start', 'End']] = df[['Start', 'End']].apply(pd.to_numeric, errors='coerce')\n",
    "df.drop(columns=['Description'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "metadata.dropna(subset=['SUBJID'], inplace=True)\n",
    "metadata.rename(columns={'SUBJID': 'Sample'}, inplace=True)\n",
    "\n",
    "# Helper functions\n",
    "def shannon_entropy(probs):\n",
    "    #Calculate Shannon entropy.\n",
    "    probs = np.array(probs, dtype=np.float64)\n",
    "    probs = probs[probs > 0]\n",
    "    return -np.sum(probs * np.log2(probs)) if len(probs) > 0 else 0\n",
    "\n",
    "def calculate_entropy_by_start(df, sample_columns):\n",
    "    #Entropy per Chromosome + Start, for each sample with >10 reads total.\n",
    "    grouped = df.groupby(['Chromosome', 'Start'])\n",
    "    entropies = []\n",
    "\n",
    "    for (chromosome, start), group in grouped:\n",
    "        for sample in sample_columns:\n",
    "            sample_reads = group[sample]\n",
    "            total_reads = sample_reads.sum()\n",
    "\n",
    "            if total_reads > 10:\n",
    "                probs = sample_reads / total_reads\n",
    "                entropy = shannon_entropy(probs.values)\n",
    "                entropies.append((chromosome, start, sample, entropy))\n",
    "\n",
    "    return pd.DataFrame(entropies, columns=['Chromosome', 'Start', 'Sample', 'Entropy'])\n",
    "\n",
    "def calculate_entropy_by_end(df, sample_columns):\n",
    "    #Entropy per Chromosome + End, for each sample with >10 reads total.\n",
    "    grouped = df.groupby(['Chromosome', 'End'])\n",
    "    entropies = []\n",
    "\n",
    "    for (chromosome, end), group in grouped:\n",
    "        for sample in sample_columns:\n",
    "            sample_reads = group[sample]\n",
    "            total_reads = sample_reads.sum()\n",
    "\n",
    "            if total_reads > 10:\n",
    "                probs = sample_reads / total_reads\n",
    "                entropy = shannon_entropy(probs.values)\n",
    "                entropies.append((chromosome, end, sample, entropy))\n",
    "\n",
    "    return pd.DataFrame(entropies, columns=['Chromosome', 'End', 'Sample', 'Entropy'])\n",
    "\n",
    "def process_entropy(entropy_df, metadata):\n",
    "    #Merge entropy data with metadata and clean columns.\"\"\"\n",
    "    entropy_df['Base_ID'] = entropy_df['Sample'].str.extract(r'(GTEX-\\w+)')\n",
    "    merged_df = entropy_df.merge(metadata, left_on='Base_ID', right_on='Sample', how='left')\n",
    "    return merged_df.rename(columns={'Sample_x': 'Sample'}).drop(columns=['Sample_y'])\n",
    "\n",
    "def calculate_sample_level_correlation(entropy_df, group_col):\n",
    "    #Calculate Spearman correlation between AGE and Entropy directly at sample level.\n",
    "    #Only for (chromosome, position) groups with ≥5 samples.\n",
    "    \n",
    "    correlations = []\n",
    "\n",
    "    for (chromosome, position), group in entropy_df.groupby(['Chromosome', group_col]):\n",
    "        if (\n",
    "            len(group) >= 5 and\n",
    "            group['AGE'].nunique() > 1 and\n",
    "            group['Entropy'].nunique() > 1\n",
    "        ):\n",
    "            correlation, _ = spearmanr(group['AGE'], group['Entropy'])\n",
    "            correlations.append((chromosome, position, correlation))\n",
    "\n",
    "    return pd.DataFrame(correlations, columns=['Chromosome', group_col, 'Spearman_Correlation'])\n",
    "\n",
    "# Main processing\n",
    "sample_columns = df.columns.difference(['Name', 'Chromosome', 'Start', 'End'])\n",
    "\n",
    "# Calculate entropy for Start and End positions\n",
    "df_entropy_start = process_entropy(calculate_entropy_by_start(df, sample_columns), metadata)\n",
    "df_entropy_end = process_entropy(calculate_entropy_by_end(df, sample_columns), metadata)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_start = calculate_sample_level_correlation(df_entropy_start, 'Start')\n",
    "correlation_end = calculate_sample_level_correlation(df_entropy_end, 'End')\n",
    "\n",
    "# Output results\n",
    "print(\"Sample-Level Spearman Correlations for Start Positions:\")\n",
    "print(correlation_start.head(10))\n",
    "\n",
    "print(\"\\nSample-Level Spearman Correlations for End Positions:\")\n",
    "print(correlation_end.head(10))\n",
    "\n",
    "\n",
    "# Add a column to distinguish the type of position\n",
    "correlation_start['Position_Type'] = 'Start'\n",
    "correlation_end['Position_Type'] = 'End'\n",
    "\n",
    "# Rename position column to generic 'Position' for merging\n",
    "correlation_start.rename(columns={'Start': 'Position'}, inplace=True)\n",
    "correlation_end.rename(columns={'End': 'Position'}, inplace=True)\n",
    "\n",
    "# Merge both into a single DataFrame\n",
    "correlation_combined = pd.concat([correlation_start, correlation_end], ignore_index=True)\n",
    "\n",
    "# Output the merged DataFrame\n",
    "print(\"\\nCombined Correlations (Start + End):\")\n",
    "print(correlation_combined.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_histogram(correlation_df, position):\n",
    "    \n",
    "    #Plot a histogram of Spearman correlation coefficients for all positions.\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(correlation_df['Spearman_Correlation'], bins='auto', kde=True, stat=\"density\", color=\"blue\", edgecolor=\"black\")\n",
    "\n",
    "    plt.title(f\"Distribution of Spearman Correlations for {position} Positions\", fontsize=14)\n",
    "    plt.xlabel(\"Spearman Correlation Value\", fontsize=12)\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot histogram for all positions\n",
    "plot_correlation_histogram(correlation_combined, \"All\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest, anderson, normaltest\n",
    "\n",
    "def perform_normality_tests_all_positions(correlation_df):\n",
    "    \n",
    "    #Perform normality tests for Spearman correlation values across all positions.\n",
    "    \n",
    "    data = correlation_df['Spearman_Correlation'].dropna()  # Remove NaNs\n",
    "    \n",
    "    if len(data) < 3:\n",
    "        print(\"Not enough data points for normality tests.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nNormality Tests for Spearman Correlation Values (All Positions):\")\n",
    "    \n",
    "    # Kolmogorov-Smirnov Test\n",
    "    ks_stat, ks_p = kstest(data, 'norm', args=(data.mean(), data.std(ddof=1)))\n",
    "    print(f\"Kolmogorov-Smirnov Test: Statistic={ks_stat:.4f}, p-value={ks_p:.4f}\")\n",
    "    \n",
    "    # Anderson-Darling Test\n",
    "    ad_result = anderson(data, dist='norm')\n",
    "    print(f\"Anderson-Darling Test: Statistic={ad_result.statistic:.4f}\")\n",
    "    print(f\"Critical Values: {ad_result.critical_values}\")\n",
    "    print(f\"Significance Levels: {ad_result.significance_level}\")\n",
    "    \n",
    "    # D'Agostino and Pearson's Test\n",
    "    dag_stat, dag_p = normaltest(data)\n",
    "    print(f\"D'Agostino and Pearson's Test: Statistic={dag_stat:.4f}, p-value={dag_p:.4f}\")\n",
    "\n",
    "# Run the normality tests on the combined correlation data\n",
    "perform_normality_tests_all_positions(correlation_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f22f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest, anderson, normaltest\n",
    "\n",
    "\n",
    "def perform_normality_tests_all_positions(correlation_df):\n",
    "    \n",
    "    #Perform normality tests for Spearman correlation values across all positions.\n",
    "    \n",
    "    data = correlation_df['Spearman_Correlation'].dropna()  \n",
    "    \n",
    "    print(f\"\\nNormality Tests for Spearman Correlation Values (All Positions):\")\n",
    "    \n",
    "    # Kolmogorov-Smirnov Test\n",
    "    ks_stat, ks_p = kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    print(f\"Kolmogorov-Smirnov Test: Statistic={ks_stat:.4f}, p-value={ks_p:.4f}\")\n",
    "    \n",
    "    # Anderson-Darling Test\n",
    "    ad_result = anderson(data, dist='norm')\n",
    "    print(f\"Anderson-Darling Test: Statistic={ad_result.statistic:.4f}\")\n",
    "    print(f\"Critical Values: {ad_result.critical_values}\")\n",
    "    print(f\"Significance Levels: {ad_result.significance_level}\")\n",
    "    \n",
    "    # D'Agostino and Pearson's Test\n",
    "    dag_stat, dag_p = normaltest(data)\n",
    "    print(f\"D'Agostino and Pearson's Test: Statistic={dag_stat:.4f}, p-value={dag_p:.4f}\")\n",
    "\n",
    "# Run the normality tests on the combined correlation data\n",
    "perform_normality_tests_all_positions(correlation_combined)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def detect_percentile_outliers_with_thresholds(df, column, percentile=0.5):\n",
    "    \n",
    "    #Detects outliers based on the upper and lower percentile thresholds.\n",
    "    #Computes and returns the actual threshold values along with the outliers.\n",
    "    \n",
    "    lower_threshold = np.percentile(df[column], percentile)\n",
    "    upper_threshold = np.percentile(df[column], 100 - percentile)\n",
    "\n",
    "    outliers = df[(df[column] < lower_threshold) | (df[column] > upper_threshold)]\n",
    "    \n",
    "    return outliers, lower_threshold, upper_threshold\n",
    "\n",
    "def plot_correlation_histogram_with_outliers(correlation_df, column, lower_threshold, upper_threshold):\n",
    "    \n",
    "    #Plot a histogram of Spearman correlation coefficients and highlight the outlier thresholds.\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Histogram of all correlation values\n",
    "    sns.histplot(correlation_df[column], bins='auto', kde=True, stat=\"density\", color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "    #Vertical lines for outlier thresholds\n",
    "    plt.axvline(lower_threshold, color='red', linestyle='dashed', linewidth=2, label=f\"Lower Threshold ({lower_threshold:.4f})\")\n",
    "    plt.axvline(upper_threshold, color='red', linestyle='dashed', linewidth=2, label=f\"Upper Threshold ({upper_threshold:.4f})\")\n",
    "\n",
    "    # Titles and labels\n",
    "    plt.title(f\"Distribution of {column} with Outlier Thresholds\", fontsize=14)\n",
    "    plt.xlabel(f\"{column} Value\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Detect outliers for Spearman correlations (All positions)\n",
    "outliers_spearman, lower_threshold, upper_threshold = detect_percentile_outliers_with_thresholds(\n",
    "    correlation_combined, 'Spearman_Correlation', percentile=0.5\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Lower Threshold (0.5%): {lower_threshold:.4f}\")\n",
    "print(f\"Upper Threshold (99,5%): {upper_threshold:.4f}\")\n",
    "print(f\"Total Outliers Detected: {outliers_spearman.shape[0]}\")\n",
    "\n",
    "# Plot histogram with outlier thresholds\n",
    "plot_correlation_histogram_with_outliers(correlation_combined, 'Spearman_Correlation', lower_threshold, upper_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd05b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pybedtools\n",
    "\n",
    "# 1. Compute global outliers \n",
    "global_outliers, global_lower, global_upper = detect_percentile_outliers_with_thresholds(\n",
    "    correlation_combined, 'Spearman_Correlation', percentile=0.5\n",
    ")\n",
    "\n",
    "# 2. Identify the correct position column dynamically\n",
    "position_col = None\n",
    "for col in global_outliers.columns:\n",
    "    if \"pos\" in col.lower() or \"start\" in col.lower():  # Covers \"Position\", \"Start_Position\", \"Start\"\n",
    "        position_col = col\n",
    "        break\n",
    "\n",
    "if position_col is None:\n",
    "    raise KeyError(\"No suitable position column (e.g., 'Start' or 'Position') found in the DataFrame.\")\n",
    "\n",
    "# 3. Compute Pseudo-End as Position + 1\n",
    "global_outliers[\"Pseudo_End\"] = global_outliers[position_col] + 1\n",
    "\n",
    "# 4. Rename columns for BED format\n",
    "global_outliers[\"End\"] = global_outliers[\"Pseudo_End\"]\n",
    "global_outliers.rename(columns={position_col: \"Start\"}, inplace=True)\n",
    "\n",
    "# 5. Ensure integer type for genomic positions\n",
    "global_outliers = global_outliers.dropna(subset=[\"Start\", \"End\"])\n",
    "global_outliers[\"Start\"] = global_outliers[\"Start\"].astype(int)\n",
    "global_outliers[\"End\"] = global_outliers[\"End\"].astype(int)\n",
    "\n",
    "# 6. Separate into lower 5% and upper 95%\n",
    "lower_outliers = global_outliers[global_outliers[\"Spearman_Correlation\"] < global_lower]\n",
    "upper_outliers = global_outliers[global_outliers[\"Spearman_Correlation\"] > global_upper]\n",
    "\n",
    "# 7. Format for BED (Chromosome, Start, End)\n",
    "lower_pseudo_outliers = lower_outliers[[\"Chromosome\", \"Start\", \"End\"]]\n",
    "upper_pseudo_outliers = upper_outliers[[\"Chromosome\", \"Start\", \"End\"]]\n",
    "\n",
    "# 8. Convert to BED format\n",
    "lower_pseudo_bed = pybedtools.BedTool.from_dataframe(lower_pseudo_outliers)\n",
    "upper_pseudo_bed = pybedtools.BedTool.from_dataframe(upper_pseudo_outliers)\n",
    "\n",
    "# 9. Load reference gene annotations\n",
    "reference_genes = pybedtools.BedTool(\"hg38_refseq_genes.bed\")\n",
    "\n",
    "# 10. Intersect lower and upper outliers with reference genes\n",
    "intersected_lower = lower_pseudo_bed.intersect(reference_genes, wa=True, wb=True).to_dataframe(\n",
    "    names=[\"Chromosome\", \"Start\", \"End\", \"Gene\"]\n",
    ")\n",
    "intersected_upper = upper_pseudo_bed.intersect(reference_genes, wa=True, wb=True).to_dataframe(\n",
    "    names=[\"Chromosome\", \"Start\", \"End\", \"Gene\"]\n",
    ")\n",
    "\n",
    "# 11. Print results\n",
    "print(\"Checking intersection results for Lower 0.5% Spearman Outliers...\")\n",
    "print(intersected_lower.head())\n",
    "\n",
    "print(\"Checking intersection results for Upper 99.5% Spearman Outliers...\")\n",
    "print(intersected_upper.head())\n",
    "\n",
    "# 12. Save results\n",
    "#intersected_lower.to_csv(\"intersected_pseudo_outliers_spearman_lower1.csv\", index=False, sep=\"\\t\")\n",
    "#intersected_upper.to_csv(\"intersected_pseudo_outliers_spearman_upper99.csv\", index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"Intersection results saved for Lower 0.5% and Upper 99.5% Spearman Outliers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gprofiler-official\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b12142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background genes from Amygdala_junctions.tsv\n",
    "background_file = file_path\n",
    "df_background = pd.read_csv(background_file, sep=\"\\t\")\n",
    "\n",
    "# Extract unique gene names from the \"Description\" column\n",
    "background_genes = df_background[\"Description\"].dropna().unique().tolist()\n",
    "\n",
    "# Remove version numbers from Ensembl IDs\n",
    "background_genes = [gene.split('.')[0] for gene in background_genes]\n",
    "\n",
    "\n",
    "print(f\" Total background genes: {len(background_genes)}\")\n",
    "print(f\" Sample background genes: {background_genes[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c53332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gprofiler import GProfiler\n",
    "\n",
    "def process_enrichment_gprofiler(intersected_df, label, background_genes):\n",
    "    #Perform gene set enrichment analysis using g:Profiler with custom background.\n",
    "    \n",
    "    # Extract unique gene names from intersected genes\n",
    "    unique_genes = intersected_df[\"Gene\"].dropna().unique().tolist()\n",
    "\n",
    "    print(f\"\\n {label} - Total unique genes: {len(unique_genes)}\")\n",
    "    print(f\" {label} - Sample genes: {unique_genes[:10]}\")\n",
    "\n",
    "    # Run g:Profiler enrichment with a custom background\n",
    "    if len(unique_genes) > 0:\n",
    "        gp = GProfiler(return_dataframe=True)\n",
    "        enrichment_results = gp.profile(\n",
    "            organism=\"hsapiens\",\n",
    "            query=unique_genes,\n",
    "            background=background_genes,  # Use custom background\n",
    "            sources=[\"GO:BP\", \"GO:MF\", \"GO:CC\", \"KEGG\", \"REAC\", \"WP\"]\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        output_file = f\"gene_enrichment_results_{label}.csv\"\n",
    "        enrichment_results.to_csv(output_file, index=False)\n",
    "        print(f\" {label} - Enrichment results saved in '{output_file}'\")\n",
    "    else:\n",
    "        print(f\" {label} - No genes found for enrichment.\")\n",
    "\n",
    "# Run enrichment analysis with g:Profiler using custom background\n",
    "process_enrichment_gprofiler(intersected_lower, \"spearman_lower_5\", background_genes)\n",
    "process_enrichment_gprofiler(intersected_upper, \"spearman_upper_95\", background_genes)\n",
    "\n",
    "print(\"\\n Enrichment analysis completed with custom background.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gprofiler import GProfiler\n",
    "\n",
    "def process_enrichment_gprofiler(intersected_df, label, background_genes):\n",
    "    #Perform gene set enrichment analysis using g:Profiler with custom background and filtering.\n",
    "    \n",
    "    # Extract unique gene names\n",
    "    unique_genes = intersected_df[\"Gene\"].dropna().unique().tolist()\n",
    "\n",
    "    print(f\"\\n {label} - Total unique genes: {len(unique_genes)}\")\n",
    "    print(f\" {label} - Sample genes: {unique_genes[:10]}\")\n",
    "\n",
    "    if len(unique_genes) > 0:\n",
    "        gp = GProfiler(return_dataframe=True)\n",
    "        enrichment_results = gp.profile(\n",
    "            organism=\"hsapiens\",\n",
    "            query=unique_genes,\n",
    "            background=background_genes,  # Use custom background\n",
    "            sources=[\"GO:BP\", \"GO:MF\", \"GO:CC\", \"KEGG\", \"REAC\", \"WP\"],\n",
    "            no_evidences=True,  # Removes very general terms\n",
    "            user_threshold=0.01  # More stringent p-value threshold\n",
    "        )\n",
    "\n",
    "        # **Manual Filtering for Large Gene Sets**\n",
    "        if not enrichment_results.empty:\n",
    "            enrichment_results = enrichment_results[\n",
    "                (enrichment_results[\"precision\"] > 0.15) &  # Keep more specific terms\n",
    "                (enrichment_results[\"recall\"] < 0.05) &  # Remove broad terms covering too many genes\n",
    "                (enrichment_results[\"intersection_size\"] < 30)  # Exclude overly large terms\n",
    "            ]  \n",
    "\n",
    "            # Save results\n",
    "            output_file = f\"gene_enrichment_results_{label}.csv\"\n",
    "            enrichment_results.to_csv(output_file, index=False)\n",
    "            print(f\" {label} - Filtered enrichment results saved in '{output_file}'\")\n",
    "        else:\n",
    "            print(f\" {label} - No significant enrichment results found.\")\n",
    "    else:\n",
    "        print(f\" {label} - No genes found for enrichment.\")\n",
    "\n",
    "# Run enrichment analysis with g:Profiler using custom background\n",
    "process_enrichment_gprofiler(intersected_lower, \"spearman_lower_5\", background_genes)\n",
    "process_enrichment_gprofiler(intersected_upper, \"spearman_upper_95\", background_genes)\n",
    "\n",
    "print(\"\\n Enrichment analysis completed with custom background and filtering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_enrichment_results(file_path, label, top_n=10):\n",
    "    #Plot top N enriched terms from g:Profiler results.\n",
    "    \n",
    "    # Load enrichment results, auto-detecting delimiter\n",
    "    df = pd.read_csv(file_path, sep=None, engine=\"python\")\n",
    "\n",
    "    # Print column names to verify correct loading\n",
    "    print(f\"\\nColumns in {file_path}: {df.columns.tolist()}\")\n",
    "\n",
    "    # Check if expected columns exist\n",
    "    expected_columns = [\"p_value\", \"name\", \"source\"]\n",
    "    missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"❌ Missing expected columns: {missing_columns}\")\n",
    "        print(f\"⚠ Check the file format and column names: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Check if the file contains any data\n",
    "    if df.empty:\n",
    "        print(f\" No enriched terms found for {label}.\")\n",
    "        return\n",
    "\n",
    "    # Sort by p-value and select the top terms\n",
    "    df = df.sort_values(by=\"p_value\", ascending=True).head(top_n)\n",
    "\n",
    "    # Plot bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        y=df[\"name\"], \n",
    "        x=-np.log10(df[\"p_value\"]), \n",
    "        hue=df[\"source\"],\n",
    "        dodge=False\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"-log10(p-value)\", fontsize=12)\n",
    "    plt.ylabel(\"Enriched Terms\", fontsize=12)\n",
    "    plt.title(f\"Top {top_n} Enriched Terms - {label}\", fontsize=14)\n",
    "    plt.legend(title=\"Source\")\n",
    "    plt.gca().invert_yaxis()  # Flip the order to show the most significant term on top\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for both lower and upper Spearman outliers\n",
    "plot_enrichment_results(\"gene_enrichment_results_spearman_lower_5.csv\", \"Lower 0.5% Outliers\")\n",
    "plot_enrichment_results(\"gene_enrichment_results_spearman_upper_95.csv\", \"Upper 99.5% Outliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b57d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
